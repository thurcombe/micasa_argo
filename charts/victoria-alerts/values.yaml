# ---- kube-state-metrics (lightweight source of pod/object status) ----
kube-state-metrics:
  # keep it in the same namespace as this release so we can use short DNS names
  fullnameOverride: kube-state-metrics
  # defaults are fine; exposes port 8080 with /metrics

# ---- VictoriaMetrics Single (tiny TSDB just for vmalert state + quick queries) ----
victoria-metrics-single:
  # Small, predictable base name so the service becomes:
  #   vmsingle-victoria-metrics-single-server
  nameOverride: vmsingle

  server:
    # Keep it ephemeral for now to avoid StorageClass drama; flip to true later if you want retention across restarts.
    persistentVolume:
      enabled: false
      # storageClassName: ""   # set when you re-enable persistence
      # size: 5Gi
    retentionPeriod: 7d
    resources:
      requests:
        cpu: 50m
        memory: 256Mi
      limits:
        cpu: 200m
        memory: 512Mi

    service:
      type: ClusterIP

    # IMPORTANT: promscrape config must be an OBJECT, not a string.
    scrape:
      enabled: true
      config:
        global:
          scrape_interval: 30s
          evaluation_interval: 30s
          scrape_timeout: 10s

        scrape_configs:
          # Only scrape kube-state-metrics; nothing else to keep it light.
          - job_name: kube-state-metrics
            static_configs:
              - targets:
                  - "kube-state-metrics:8080"

# ---- VMAlert (executes rules, writes state to VM, sends to your Alertmanager) ----
victoria-metrics-alert:
  nameOverride: vmalert

  # vmalert needs a datasource (Prom-compatible API). Use the VM Single service name from this chart:
  #   <nameOverride>-victoria-metrics-single-server
  datasource:
    url: "http://vmsingle-victoria-metrics-single-server:8428"

  # Persist rule results & alert state; VM’s remote_write endpoint
  remoteWrite:
    url: "http://vmsingle-victoria-metrics-single-server:8428/api/v1/write"

  # (Optional) allow remote reads for recording rules reuse
  remoteRead:
    url: "http://vmsingle-victoria-metrics-single-server:8428"

  # Point at your existing Alertmanager service
  notifier:
    url: "http://alertmanager.alertmanager.svc:9093"

  extraArgs:
    # Faster feedback loop without being noisy
    evaluationInterval: "30s"

  # Inline rules — only pod health stuff; lean and focused.
  config:
    groups:
      - name: k8s.pods.core
        rules:
          # Pod Pending too long
          - alert: KubePodPendingTooLong
            expr: max by (namespace, pod) (kube_pod_status_phase{phase="Pending"}) > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Pod pending >10m"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been Pending for over 10 minutes."

          # Pod not Ready
          - alert: KubePodNotReady
            expr: max by (namespace, pod) (kube_pod_status_ready{condition="true"}) == 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod not Ready >5m"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not Ready for >5 minutes."

          # CrashLoopBackOff
          - alert: KubePodCrashLoopBackOff
            expr: sum by (namespace, pod) (kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"}) > 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "CrashLoopBackOff"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is CrashLoopBackOff for >5 minutes."

          # ImagePullBackOff
          - alert: KubePodImagePullBackOff
            expr: sum by (namespace, pod) (kube_pod_container_status_waiting_reason{reason="ImagePullBackOff"}) > 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "ImagePullBackOff"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} failing to pull image."

          # OOMKilled
          - alert: KubePodOOMKilled
            expr: sum by (namespace, pod) (kube_pod_container_status_terminated_reason{reason="OOMKilled"}) > 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "OOMKilled"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} had a container OOMKilled."
