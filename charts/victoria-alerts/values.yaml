# --- kube-state-metrics (metrics source for pod state) ---
kube-state-metrics:
  fullnameOverride: kube-state-metrics

# --- VictoriaMetrics Single (TSDB; just scraping KSM) ---
victoria-metrics-single:
  nameOverride: vmsingle
  server:
    persistentVolume:
      enabled: true
      storageClassName: local-path
      size: 1Gi
    retentionPeriod: 7d
    resources:
      requests:
        cpu: 50m
        memory: 256Mi
      limits:
        cpu: 200m
        memory: 512Mi
    service:
      type: ClusterIP
    scrape:
      enabled: true
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 10s
        scrape_configs:
          - job_name: "kube-state-metrics"
            static_configs:
              - targets: ["kube-state-metrics:8080"]

# --- VMAlert (evaluates rules, alerts to your existing Alertmanager) ---
victoria-metrics-alert:
  server:
    fullnameOverride: vmalert

    # Read from VM Single (keep your service name)
    datasource:
      url: "http://victoria-alerts-vmsingle-server:8428"

    # Alertmanager wiring
    notifier:
      alertmanager:
        url: "http://alertmanager.alertmanager.svc:9093"

    extraArgs:
      evaluationInterval: 30s

    # Lean rules using only KSM. Completed pods are avoided; CrashLooping is windowed to stop flapping.
    config:
      alerts:
        groups:
          - name: zfs
            interval: 30s
            rules:
              - alert: ZpoolDegraded
                # Preferred: pdf/zfs_exporter exposes a pool metric with a "health" label.
                expr: count by (pool, instance, health) (zfs_pool_health{health!="SOMETHING"}) > 0
                # If your install exposes "health" as a property label instead, use this instead:
                # expr: count by (pool, instance, health) (zfs_pool_property{property="health", health!="ONLINE"}) > 0
                for: 2m
                labels:
                  severity: critical
                annotations:
                  summary: "ZFS pool {{ $labels.pool }} degraded on {{ $labels.instance }}"
                  description: "health={{ $labels.health }} (expected ONLINE)"
          - name: k8s.pods.core
            rules:
              # CrashLoopBackOff (windowed to prevent flapping across brief 'Completed' hops)
              - alert: KubePodCrashLoopBackOff
                expr: |
                  max_over_time(
                    kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"}[10m]
                  ) >= 1
                for: 5m
                labels: { severity: critical }
                annotations:
                  summary: "CrashLoopBackOff"
                  description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in CrashLoopBackOff within the last 10m."

              # ImagePullBackOff (stable: only if observed during the last 10m)
              - alert: KubePodImagePullBackOff
                expr: |
                  max_over_time(
                    kube_pod_container_status_waiting_reason{reason="ImagePullBackOff"}[10m]
                  ) >= 1
                for: 5m
                labels: { severity: warning }
                annotations:
                  summary: "ImagePullBackOff"
                  description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} had ImagePullBackOff within the last 10m."

              # Pending too long (naturally excludes Succeeded)
              - alert: KubePodPendingTooLong
                expr: max by (namespace, pod) (kube_pod_status_phase{phase="Pending"}) > 0
                for: 15m
                labels: { severity: warning }
                annotations:
                  summary: "Pod pending >15m"
                  description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} pending >15m."

              # Not Ready (only consider pods that have actually been Running recently)
              - alert: KubePodNotReady
                expr: |
                  (max by (namespace, pod) (kube_pod_status_ready{condition="true"} == 0) > 0)
                  and on (namespace, pod)
                  (max_over_time(kube_pod_status_phase{phase="Running"}[10m]) == 1)
                for: 10m
                labels: { severity: warning }
                annotations:
                  summary: "Pod not Ready >10m"
                  description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not Ready for >10m."

              # OOMKilled (ignore pods that purely Completed; only alert when active pod had OOM in recent window)
              - alert: KubePodOOMKilled
                expr: |
                  max_over_time(
                    kube_pod_container_status_terminated_reason{reason="OOMKilled"}[10m]
                  ) >= 1
                for: 5m
                labels: { severity: critical }
                annotations:
                  summary: "OOMKilled"
                  description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} had container(s) OOMKilled within the last 10m."
