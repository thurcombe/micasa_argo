# --- kube-state-metrics (metrics source for pod state) ---
kube-state-metrics:
  # Stable name so the Service is just "kube-state-metrics" on port 8080
  fullnameOverride: kube-state-metrics

# --- VictoriaMetrics Single (tiny TSDB; only scraped KSM) ---
victoria-metrics-single:
  # Ensures the Service name becomes: vmsingle-victoria-metrics-single-server
  nameOverride: vmsingle

  server:
    # keep ephemeral for now; flip to true + storageClassName later if you want persistence
    persistentVolume:
      enabled: true
      storageClassName: local-path
      size: 1Gi

    retentionPeriod: 7d

    resources:
      requests:
        cpu: 50m
        memory: 256Mi
      limits:
        cpu: 200m
        memory: 512Mi

    service:
      type: ClusterIP

    scrape:
      enabled: true
      # IMPORTANT: must be a YAML OBJECT (not a block string)
      config:
        global:
          scrape_interval: 30s
          scrape_timeout: 10s
        scrape_configs:
          - job_name: "kube-state-metrics"
            static_configs:
              - targets: ["kube-state-metrics:8080"]

# --- VMAlert (evaluates rules, alerts to your existing Alertmanager) ---
victoria-metrics-alert:
  server:
    # Optional, but keeps names short
    fullnameOverride: vmalert

    # Correct keys per chart docs:
    # vmalert reads from VM Singleâ€™s Prom-compatible API
    datasource:
      url: "http://victoria-alerts-vmsingle-server:8428"

    # (Optional) persist recording rules / alert state; not required for alerting-only
    remote:
      write:
        url: "http://victoria-alerts-vmsingle-server:8428/api/v1/write"
      read:
        url: "http://victoria-alerts-vmsingle-server:8428"

    # Send to your existing Alertmanager Service
    notifier:
      alertmanager:
        url: "http://alertmanager.alertmanager.svc:9093"

    # vmalert flag -> evaluate every 30s
    extraArgs:
      evaluationInterval: 30s

    # Inline alert rules using ONLY kube-state-metrics metrics
    config:
      alerts:
        groups:
          - name: k8s.pods.core
            rules:
              - alert: KubePodCrashLoopBackOff
                expr: sum by (namespace, pod) (kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"}) > 0
                for: 5m
                labels: { severity: critical }
                annotations:
                  summary: "CrashLoopBackOff"
                  description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} in CrashLoopBackOff >5m."

              - alert: KubePodImagePullBackOff
                expr: sum by (namespace, pod) (kube_pod_container_status_waiting_reason{reason="ImagePullBackOff"}) > 0
                for: 5m
                labels: { severity: warning }
                annotations:
                  summary: "ImagePullBackOff"
                  description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} failing to pull image."

              - alert: KubePodPendingTooLong
                expr: max by (namespace, pod) (kube_pod_status_phase{phase="Pending"}) > 0
                for: 10m
                labels: { severity: warning }
                annotations:
                  summary: "Pod pending >10m"
                  description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} pending >10m."

              - alert: KubePodNotReady
                expr: max by (namespace, pod) (kube_pod_status_ready{condition="true"}) == 0
                for: 5m
                labels: { severity: warning }
                annotations:
                  summary: "Pod not Ready >5m"
                  description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not Ready >5m."

              - alert: KubePodOOMKilled
                expr: sum by (namespace, pod) (kube_pod_container_status_terminated_reason{reason="OOMKilled"}) > 0
                for: 1m
                labels: { severity: critical }
                annotations:
                  summary: "OOMKilled"
                  description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} had container(s) OOMKilled."
